# @package _global_

wandb: false

name: MIMO-vit-release # load the checkpoint
benchmark: video_train
mode: test

dataset:
  name: demo
  # for RoPE inference, clip_len can be arbitrarily long, constrained by memory
  # reduced from 500 to 32 to fit in 6GB VRAM
  clip_len: 32
  # overlap length is twice the attention window size
  overlap_len: 16
  batch_size: 1

model:
  task: video

  use_smoother: true
  
  mode: inference 
  motion_ckpt: null
  image_ckpt: null

  test:
    # render: true
    save_result: true

demo:
  name: null # specify name of the demo video
  video_path: datasets/mask_transformer/EgoBody/kinect_color/recording_20220318_S33_S34_01/sub_3 # path to custom video
  output_path: null
  verbose: false
  use_bbox: true
  focal_length: null # if null, will estimate via HumanFOV

  render: true
  render_global: true